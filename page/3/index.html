
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Edwin Chen's Blog</title>
  <meta name="author" content="Edwin Chen">

  
  <meta name="description" content="How do you know what machine learning algorithm to choose for your classification problem? Of course, if you really care about accuracy, your best &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.echen.me/page/3">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/feed/" rel="alternate" title="Edwin Chen's Blog" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-29005692-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Edwin Chen's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/feed/" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.echen.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/27/choosing-a-machine-learning-classifier/">Choosing a Machine Learning Classifier</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-04-27T18:43:15-07:00" pubdate data-updated="true">Apr 27<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>How do you know what machine learning algorithm to choose for your classification problem? Of course, if you really care about accuracy, your best bet is to test out a couple different ones (making sure to try different parameters within each algorithm as well), and select the best one by cross-validation. But if you&#8217;re simply looking for a &#8220;good enough&#8221; algorithm for your problem, or a place to start, here are some general guidelines I&#8217;ve found to work well over the years.</p>

<h1>How large is your training set?</h1>

<p>If your training set is small, high bias/low variance classifiers (e.g., Naive Bayes) have an advantage over low bias/high variance classifiers (e.g., kNN), since the latter will overfit. But low bias/high variance classifiers start to win out as your training set grows (they have lower asymptotic error), since high bias classifiers aren&#8217;t powerful enough to provide accurate models.</p>

<p>You can also think of this as a generative model vs. discriminative model distinction.</p>

<h1>Advantages of some particular algorithms</h1>

<p><strong>Advantages of Naive Bayes:</strong> Super simple, you&#8217;re just doing a bunch of counts. If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. And even if the NB assumption doesn&#8217;t hold, a NB classifier still often does a great job in practice. A good bet if  want something fast and easy that performs pretty well. Its main disadvantage is that it can&#8217;t learn interactions between features (e.g., it can&#8217;t learn that although you love movies with Brad Pitt and Tom Cruise, you hate movies where they&#8217;re together).</p>

<p><strong>Advantages of Logistic Regression:</strong> Lots of ways to regularize your model, and you don&#8217;t have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you&#8217;re unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.</p>

<p><strong>Advantages of Decision Trees:</strong> Easy to interpret and explain (for some people &#8211; I&#8217;m not sure I fall into this camp). They easily handle feature interactions and they&#8217;re non-parametric, so you don&#8217;t have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). One disadvantage is that they don&#8217;t support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit, but that&#8217;s where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they&#8217;re fast and scalable, and you don&#8217;t have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.</p>

<p><strong>Advantages of SVMs:</strong> High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you&#8217;re data isn&#8217;t linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive, hard to interpret, and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.</p>

<h1>But&#8230;</h1>

<p>Recall, though, that better data often beats better algorithms, and designing good features goes a long way. And if you have a huge dataset, then whichever classification algorithm you use might not matter so much in terms of classification performance (so choose your algorithm based on speed or ease of use instead).</p>

<p>And to reiterate what I said above, if you really care about accuracy, you should definitely try a bunch of different classifiers and select the best one by cross-validation. Or, to take a lesson from the Netflix Prize (and Middle Earth), just use an ensemble method to choose them all.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/25/kickstarter-data-analysis-success-and-pricing/">Kickstarter Data Analysis: Success and Pricing</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-04-25T21:19:40-07:00" pubdate data-updated="true">Apr 25<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.kickstarter.com/">Kickstarter</a> is an online crowdfunding platform for launching creative projects. When starting a new project, project owners specify a deadline and the minimum amount of money they need to raise. They receive the money (less a transaction fee) only if they reach or exceed that minimum; otherwise, no money changes hands.</p>

<p>What&#8217;s particularly fun about Kickstarter is that in contrast to <a href="http://www.kiva.org/">that other microfinance site</a>, Kickstarter projects don&#8217;t ask for loans; instead, patrons receive pre-specified rewards unique to each project. For example, someone donating money to help an artist record an album might receive a digital copy of the album if they donate 20 dollars, or a digital copy plus a signed physical cd if they donate 50 dollars.</p>

<p>There are <a href="http://www.kickstarter.com/discover/hall-of-fame?ref=sidebar">a bunch</a> of <a href="http://www.kickstarter.com/projects/1104350651/tiktok-lunatik-multi-touch-watch-kits">neat</a> <a href="http://www.kickstarter.com/projects/2024077040/neil-gaimans-the-price">projects</a>, and I&#8217;m tempted to put one of my own on there soon, so I thought it would be fun to gather some data from the site and see what makes a project successful.</p>

<h1>Categories</h1>

<p>I started by scraping the categories section.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-projects-by-category.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-projects-by-category.png" alt="Successful projects by category" /></a></p>

<p>In true indie fashion, the artsy categories tend to dominate. (I&#8217;m surprised/disappointed how little love the Technology category gets.)</p>

<h1>Ending Soon</h1>

<p>The categories section really only provides a history of <em>successful</em> projects, though, so to get some data on unsuccessful projects as well, I took a look at the <a href="http://www.kickstarter.com/discover/ending-soon?ref=sidebar">Ending Soon</a> section of projects whose deadlines are about to pass.</p>

<p>It looks like about 50% of all Kickstarter projects get successfully funded by the deadline:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/ending-soon-success.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/ending-soon-success.png" alt="Successful projects as deadline approaches" /></a></p>

<p>Interestingly, most of the final funding seems to happen in the final few days: with just 5 days left, only about 20% of all projects have been funded. (In other words, with just 5 days left, 60% of the projects that will eventually be successful are still unfunded.) So the approaching deadline seems to really spur people to donate. I wonder if it&#8217;s because of increased publicity in the final few days (the project owners begging everyone for help!) or if it&#8217;s simply procrastination in action (perhaps people want to wait to see if their donation is really necessary)?</p>

<p>Lesson: if you&#8217;re still not fully funded with only a couple days remaining, don&#8217;t despair.</p>

<h1>Success vs. Failure</h1>

<p>What factors lead a project to succeed? Are there any quantitative differences between projects that eventually get funded and those that don&#8217;t?</p>

<p>Two simple (if kind of obvious) things I noticed are that unsuccessful projects tend to require a larger amount of money:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-goal.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-goal.png" alt="Unsuccessful projects tend to ask for more money" /></a></p>

<p>and unsuccessful projects also tend to raise less money in absolute terms (i.e., it&#8217;s not just that they ask for too much money to reach their goal &#8211; they&#8217;re simply not receiving enough money as well):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-amount-pledged.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-amount-pledged.png" alt="Unsuccessful projects received less money" /></a></p>

<p>Not terribly surprising, but it&#8217;s good to confirm (and I&#8217;m still working on finding other predictors).</p>

<h1>Pledge Rewards</h1>

<p>There&#8217;s a lot of interesting work in behavioral economics on pricing and choice &#8211; for example, the <a href="http://youarenotsosmart.com/2010/07/27/anchoring-effect/">anchoring effect</a> suggests that when building a menu, you should <a href="http://www.neurosciencemarketing.com/blog/articles/neuro-menus-and-restaurant-psychology.htm">include an expensive item</a> to make other menu items look reasonably priced in comparison, and the <a href="http://en.wikipedia.org/wiki/The_Paradox_of_Choice:_Why_More_Is_Less">paradox of choice </a> suggests that too many choices lead to a decision freeze &#8211; so one aspect of the Kickstarter data I was especially interested in was how pricing of rewards affects donations. For example, does pricing the lowest reward at 25 dollars lead to more money donated (people don&#8217;t lowball at 5 dollars instead) or less money donated (25 dollars is more money than most people are willing to give)? And what happens if a new reward at 5 dollars is added &#8211; again, does it lead to more money (now people can donate something they can afford) or less money (the people that would have paid 25 dollars switch to a 5 dollar donation)?</p>

<p>First, here&#8217;s a look at the total number of pledges at each price. (More accurately, it&#8217;s the number of claimed rewards at each price.) [Update: the original version of this graph was wrong, but I&#8217;ve since fixed it.]</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/pledge%20amounts.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/pledge%20amounts.png" alt="Pledge Amounts" /></a></p>

<p>Surprisingly, 5 dollar and 1 dollar donations are actually not the most common contribution.</p>

<p>To investigate pricing effects, I started by looking at all (successful) projects that had a reward priced at 1 dollar, and compared the number of donations at 1 dollar with the number of donations at the next lowest reward.</p>

<p>Up to about 15-20 dollars, there&#8217;s a steady increase in the proportion of people who choose the second reward over the first reward, but after that, the proportion decreases.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring.png" alt="Anchoring" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring-abline-b.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring-abline-b.png" alt="Anchoring with Regression Lines" /></a></p>

<p>So this perhaps suggests that if you&#8217;re going to price your lowest reward at 1 dollar, your next reward should cost roughly 20 dollars (or slightly more, to maximize your total revenue). Pricing above 20 dollars is a little too expensive for the folks who want to support you, but aren&#8217;t rich enough to throw gads of money; maybe rewards below 20 dollars aren&#8217;t good enough to merit the higher donation.</p>

<p>Next, I&#8217;m planning on digging a little deeper into pricing effects and what makes a project successful, so I&#8217;ll hopefully have some more Kickstarter analysis in a future post. In the meantime, in case anyone else wants to take a look, I put the data onto <a href="https://github.com/echen/kickstarter-data-analysis">my Github account</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/21/a-mathematical-introduction-to-least-angle-regression/">A Mathematical Introduction to Least Angle Regression</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-04-21T00:16:36-07:00" pubdate data-updated="true">Apr 21<span>st</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>(For a layman&#8217;s introduction, see <a href="http://blog.echen.me/2011/03/14/least-angle-regression-for-the-hungry-layman/">here</a>.)</p>

<p>Least Angle Regression (aka LARS) is a <strong>model selection method</strong> for linear regression (when you&#8217;re worried about overfitting or want your model to be easily interpretable). To motivate it, let&#8217;s consider some other model selection methods:</p>

<ul>
<li><strong>Forward selection</strong> starts with no variables in the model, and at each step it adds to the model the variable with the most explanatory power, stopping if the explanatory power falls below some threshold. This is a fast and simple method, but it can also be too greedy: we fully add variables at each step, so correlated predictors don&#8217;t get much of a chance to be included in the model. (For example, suppose we want to build a model for the deliciousness of a PB&amp;J sandwich, and two of our variables are the amount of peanut butter and the amount of jelly. We&#8217;d like both variables to appear in our model, but since amount of peanut butter is (let&#8217;s assume) strongly correlated with the amount of jelly, once we fully add peanut butter to our model, jelly doesn&#8217;t add much explanatory power anymore, and so it&#8217;s unlikely to be added.)</li>
<li><strong>Forward stagewise regression</strong> tries to remedy the greediness of forward selection by only partially adding variables. Whereas forward selection finds the variable with the most explanatory power and goes all out in adding it to the model, forward stagewise finds the variable with the most explanatory power and updates its weight by only epsilon in the correct direction. (So we might first increase the weight of peanut butter a little bit, then increase the weight of peanut butter again, then increase the weight of jelly, then increase the weight of bread, and then increase the weight of peanut butter once more.) The problem now is that we have to make a ton of updates, so forward stagewise can be very inefficient.</li>
</ul>


<p>LARS, then, is essentially forward stagewise made fast. Instead of making tiny hops in the direction of one variable at a time, LARS makes optimally-sized leaps in optimal directions. These directions are chosen to make equal angles (equal correlations) with each of the variables currently in our model. (We like peanut butter best, so we start eating it first; as we eat more, we get a little sick of it, so jelly starts looking equally appetizing, and we start eating peanut butter and jelly simultaneously; later, we add bread to the mix, etc.)</p>

<p>In more detail, LARS works as follows:</p>

<ul>
<li>Assume for simplicity that we&#8217;ve standardized our explanatory variables to have zero mean and unit variance, and that our response variable also has zero mean.</li>
<li>Start with no variables in your model.</li>
<li>Find the variable $ x_1 $ most correlated with the residual. (Note that the variable most correlated with the residual is equivalently the one that makes the least angle with the residual, whence the name.)</li>
<li>Move in the direction of this variable until some other variable $ x_2 $ is just as correlated.</li>
<li>At this point, start moving in a direction such that the residual stays equally correlated with $ x_1 $ and $ x_2 $ (i.e., so that the residual makes equal angles with both variables), and keep moving until some variable $ x_3 $ becomes equally correlated with our residual.</li>
<li>And so on, stopping when we&#8217;ve decided our model is big enough.</li>
</ul>


<p>For example, consider the following image (slightly simplified from the <a href="http://www.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">original LARS paper</a>; $x_1, x_2$ are our variables, and $y$ is our response):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/lars/lars-example.png"><img src="http://dl.dropbox.com/u/10506/blog/lars/lars-example.png" alt="LARS Example" /></a></p>

<p>Our model starts at $ \hat{\mu_0} $.</p>

<ul>
<li>The residual (the green line) makes a smaller angle with $ x_1 $ than with $ x_2 $, so we start moving in the direction of $ x_1 $.
At $ \hat{\mu_1} $, the residual now makes equal angles with $ x_1, x_2 $, and so we start moving in a new direction that preserves this equiangularity/equicorrelation.</li>
<li>If there were more variables, we&#8217;d change directions again once a new variable made equal angles with our residual, and so on.</li>
</ul>


<p>So when should you use LARS, as opposed to some other regularization method like lasso? There&#8217;s not really a clear-cut answer, but LARS tends to give very similar results as both lasso and forward stagewise (in fact, slight modifications to LARS give you lasso and forward stagewise), so I tend to just use lasso when I do these kinds of things, since the justifications for lasso make a little more sense to me. In fact, I don&#8217;t usually even think of LARS as a model selection method in its own right, but rather as a way to efficiently implement lasso (especially if you want to compute the full regularization path).</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/18/twifferences-between-californians-and-new-yorkers/">Twifferences Between Californians and New Yorkers</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-04-18T03:41:39-07:00" pubdate data-updated="true">Apr 18<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>How do people in Silicon Valley compare to those in New York City? For a data-driven answer, I pulled a couple days&#8217; worth of Twitter data around the two areas.</p>

<h2>Silicon Valley vs. NYC</h2>

<p>So let&#8217;s take a look. To compare the two places, I trained a simple Naive Bayes model on user profiles. Here are the words most predictive of being from Silicon Valley vs. NYC:</p>

<h3>Silicon Valley</h3>

<pre><code>word                    Prob(Silicon Valley | word)
cloud                   0.9053263586466359
stanford                0.8947378473880241
hacker                  0.8759014372329466
dev                     0.8684686297323906
giants                  0.8522440881548609
mac                     0.8453052631578948
support                 0.8453052631578948
ux                      0.8453052631578948
engineer                0.8321693229113796
software                0.8223647630482606
google                  0.8199372672025093
yahoo                   0.8095254553623891
ui                      0.7978356566397651
board                   0.7978356566397651
programmer              0.7978356566397651
health                  0.7946855178965604
game                    0.7846171884159925
bike                    0.7846171884159925
gadgets                 0.7846171884159925
surfer                  0.7846171884159925
lead                    0.7846171884159925
green                   0.7758225129356634
product                 0.7707827422373662
engineering             0.7662790401791395
data                    0.7662790401791395
biz                     0.7522143788498717
games                   0.7503363731499477
cto                     0.7474653744372272
startup                 0.7474653744372272
cofounder               0.745373899785155 
team                    0.7399904460289096
mobile                  0.7395369397321055
entrepreneur            0.7346495409451503
before                  0.7320595099183198
corporate               0.7320595099183198
companies               0.7320595099183198
enterprise              0.7320595099183198
careers                 0.7320595099183197
help                    0.7320595099183197
jobs                    0.7226147417213892
systems                 0.7183120188916491
dude                    0.7183120188916491
solutions               0.7146518581802648
startups                0.7146518581802648
instructor              0.7146518581802648
founder                 0.7136727946906866
advocate                0.7119786767011602
community               0.7107516339869281
local                   0.7083355385022493
cyclist                 0.7083355385022493
formerly                0.7083355385022493
tech                    0.7034995094145681
dad                     0.699465571148113 
developer               0.6933557437157872
evangelist              0.6886440677966102
architect               0.6886440677966102
experience              0.6875894616006009
designer                0.6861009534875773
app                     0.6861009534875773
gamer                   0.6701428216249068
geek                    0.6699839438296025
friend                  0.6692937011181566
married                 0.6692937011181566
past                    0.6670653907496013
yoga                    0.6670653907496013
baseball                0.6670653907496013
iphone                  0.65665476702241  
guy                     0.6396064208138196
running                 0.6323876017254413
</code></pre>

<h3>New York City</h3>

<pre><code>word                    Prob(New York | word)
bbm                     0.9235374771480804
mets                    0.9165294616574365
teamfollowback          0.9165294616574365
nyu                     0.9165294616574365
columbia                0.8848747591522158
actress                 0.8682158330051202
chick                   0.845920058942715 
theatre                 0.8010386109569492
money                   0.7935205183585313
publishing              0.7854119457864808
moment                  0.7854119457864808
self                    0.7854119457864808
ya                      0.7545697268432944
intern                  0.7545697268432944
makeup                  0.7512175173798283
financial               0.7512175173798283
studio                  0.7329822041337483
records                 0.7329822041337483
magazine                0.7272706101537029
history                 0.7249169653720938
fashion                 0.718105290117341 
myself                  0.7118622174381054
soul                    0.7072308553828924
peace                   0.7072308553828924
dancer                  0.7017521519889981
night                   0.687114269683935 
president               0.6871142696839349
event                   0.6577157178660937
dreams                  0.6577157178660937
model                   0.6577157178660937
read                    0.6577157178660937
journalism              0.6577157178660937
filmmaker               0.6530588061027995
news                    0.6466488313151225
mother                  0.6466488313151224
entertainment           0.6466488313151224
party                   0.640842613712599 
political               0.640842613712599 
producer                0.6372663804691904
communications          0.6222197132211811
actor                   0.6169228655604573
journalist              0.616094753936313 
website                 0.6133083363295495
radio                   0.6133083363295495
executive               0.6058707124010555
</code></pre>

<p>Some findings:</p>

<ul>
<li>Silicon Valley is much more <strong>tech</strong>- (<em>cloud, hacker, dev, engineer, software, programmer</em>) and <strong>startup</strong>-driven (<em>startup, entrepreneur, founder</em>). New York is much more into <strong>arts/entertainment</strong> (<em>actress, theatre, publishing, studio, records, magazine, journalism, filmmaker, news</em>) and <strong>finance</strong> (<em>money, financial</em>). (Another way to look at it: Silicon Valley higher-ups call themselves <em>ctos</em> and <em>founders</em> and sit on <em>boards</em>, while New York higher-ups call themselves <em>presidents</em> and <em>executives</em>.)</li>
<li>New Yorkers care a lot about <strong>looks</strong> (<em>makeup, fashion</em>). Silicon Valley folks care a lot about <strong>sports</strong>, <strong>health</strong>, and the <strong>outdoors</strong> (<em>bike, surfer, cyclist, yoga, running, health</em>).</li>
<li>Silicon Valley folks describe themselves through their <strong>past</strong> (<em>before, formerly, past, former</em>). New Yorkers describe themselves through their <strong>future</strong> (<em>dreams</em>).</li>
<li>Silicon Valley is big on <strong>community</strong> (<em>support, team, help, helping, community</em>). New York is focused on the <strong>individual</strong> (<em>self, myself</em>).</li>
<li>Silicon Valley has a larger proportion of <strong>men</strong> (<em>dad, geek, dude, guy</em>). New York has the <strong>women</strong> (<em>mother, chick</em>).</li>
<li>New York uses BlackBerrys (<em>bbm</em> is short for BlackBerry Messenger), while Silicon Valley uses <em>iPhones</em>.</li>
<li>Silicon Valley is more into the <strong>locavore</strong> movement (<em>green, local</em>).</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/16/what-is-cointegration-2/">Introduction to Cointegration and Pairs Trading</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-04-16T03:44:14-07:00" pubdate data-updated="true">Apr 16<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>Introduction</h1>

<p>Suppose you see two drunks (i.e., two random walks) wandering around. The drunks don&#8217;t know each other (they&#8217;re independent), so there&#8217;s no meaningful relationship between their paths.</p>

<p>But suppose instead you have a drunk walking with her dog. This time there <em>is</em> a connection. What&#8217;s the nature of this connection? Notice that although each path individually is still an unpredictable random walk, given the location of one of the drunk or dog, we have a pretty good idea of where the other is; that is, the distance between the two is fairly predictable. (For example, if the dog wanders too far away from his owner, she&#8217;ll tend to move in his direction to avoid losing him, so the two stay close together despite a tendency to wander around on their own.) We describe this relationship by saying that the drunk and her dog form a cointegrating pair.</p>

<p>In more technical terms, if we have two non-stationary time series X and Y that become stationary when differenced (these are called integrated of order one series, or I(1) series; random walks are one example) such that some linear combination of X and Y is stationary (aka, I(0)), then we say that X and Y are cointegrated. In other words, while neither X nor Y alone hovers around a constant value, some combination of them does, so we can think of cointegration as describing a particular kind of long-run equilibrium relationship. (The definition of cointegration can be extended to multiple time series, with higher orders of integration.)</p>

<p>Other examples of cointegrated pairs:</p>

<ul>
<li>Income and consumption: as income increases/decreases, so too does consumption.</li>
<li>Size of police force and amount of criminal activity</li>
<li>A book and its movie adaptation: while the book and the movie may differ in small details, the overall plot will remain the same.</li>
<li>Number of patients entering or leaving a hospital</li>
</ul>


<h1>An application</h1>

<p>So why do we care about cointegration? In quantitative finance, cointegration forms the basis of the pairs trading strategy: suppose we have two cointegrated stocks X and Y, with the particular (for concreteness) cointegrating relationship X - 2Y = Z, where Z is a stationary series of zero mean. For example, X could be McDonald&#8217;s, Y could be Burger King, and the cointegration relationship would mean that X tends to be priced twice as high as Y, so that when X is more than twice the price of Y, we expect X to move down or Y to move up in the near future (and analogously, if X is less than twice the price of Y, we expect X to move up or Y to move down). This suggests the following trading strategy: if X - 2Y > d, for some positive threshold d, then we should sell X and buy Y (since we expect X to decrease in price and Y to increase), and similarly, if X - 2Y &lt; -d, then we should buy X and sell Y.</p>

<h1>Spurious regression</h1>

<p>But why do we need the notion of cointegration at all? Why can&#8217;t we simply use, say, the R-squared between X or Y to see if X and Y have some kind of relationship? The reason is that standard regression analysis fails when dealing with non-stationary variables, leading to spurious regressions that suggest relationships even when there are none.</p>

<p>For example, suppose we regress two independent random walks against each other, and test for a linear relationship. A large percentage of the time, we&#8217;ll find high R-squared values and low p-values when using standard OLS statistics, even though there&#8217;s absolutely no relationship between the two random walks. As an illustration, here I simulated 1000 pairs of random walks of length 100, and found p-values less than 0.05 in 77% of the cases:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/cointegration/spurious-regression.png"><img src="http://dl.dropbox.com/u/10506/blog/cointegration/spurious-regression.png" alt="Spurious Regression" /></a></p>

<h1>A Cointegration Test</h1>

<p>So how do you detect cointegration? There are several different methods, but the simplest is the Engle-Granger test, which works roughly as follows:</p>

<ul>
<li>Check that $ X_t $ and $ Y_t $ are both I(1).</li>
<li>Estimate the cointegrating relationship $ Y_t = aX_t + e_t $ by ordinary least squares.</li>
<li>Check that the cointegrating residuals $ e_t $ are stationary (say, by using a so-called unit root test, e.g., the Dickey-Fuller test).</li>
</ul>


<h1>Error-correction and Granger representation</h1>

<p>Something else that should perhaps be mentioned is the relationship between cointegration and error-correction mechanisms: suppose we have two cointegrated series $ X_t, Y_t $, with autoregressive representations</p>

<p>$ X_t = a X_{t-1} + b Y_{t-1} + u_t $
$ Y_t = c X_{t-1} + d Y_{t-1} + v_t $</p>

<p>By the Granger representation theorem (which is actually a bit more general than this), we then have</p>

<p>$ \Delta X_t = \alpha_1 (Y_{t-1} - \beta X_{t-1}) + u_t $
$ \Delta Y_t = \alpha_2 (Y_{t-1} - \beta X_{t-1}) + v_t $</p>

<p>where $ Y_{t-1} - \beta X_{t-1} \sim I(0) $ is the cointegrating relationship. Regarding $ Y_{t-1} - \beta X_{t-1} $ as the extent of disequilibrium from the long-run relationship, and the $ \alpha_i $ as the speed (and direction) at which the time series correct themselves from this disequilibrium, we can see that this formalizes the way cointegrated variables adjust to match their long-run equilbrium.</p>

<h1>Summary</h1>

<p>So, just to summarize a bit, cointegration is an equilibrium relationship between time series that individually aren&#8217;t in equilbrium (you can kind of contrast this with (Pearson) correlation, which describes a linear relationship), and it&#8217;s useful because it allows us to incorporate both short-term dynamics (deviations from equilibrium) and long-run expectations (corrections to equilibrium).</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/03/22/harry-potter-and-the-keyword-chaos-unsupervised-statistically-significant-phrases/">Unsupervised Statistically Significant Phrases</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-22T04:20:23-07:00" pubdate data-updated="true">Mar 22<span>nd</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>One of my all-time favorite graphs is the following visualization of the distribution of the primes (showing that the primes are more spread out than the random, clumpy points of a Poisson process):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/harry-potter-chaos/prime-chaos.png"><img src="http://dl.dropbox.com/u/10506/blog/harry-potter-chaos/prime-chaos.png" alt="Distribution of Primes" /></a></p>

<p>So I was pretty intrigued when I ran across <a href="http://bioinfo2.ugr.es/Publicaciones/PRE09.pdf">this paper</a> (<a href="http://bioinfo2.ugr.es/Publicaciones/PRE09.pdf">Level statistics of words: Finding keywords in literary texts and symbolic sequences</a>) using similar quantumish, random matrixish ideas to extract the important phrases from a text.</p>

<h1>The Paper</h1>

<p>Statistical phrase extraction (e.g., the &#8220;statistically improbable phrases&#8221; you see on Amazon) is usually done by comparing a target text against a baseline corpus. The authors of this paper, however, describe an <em>unsupervised</em> phrase extraction method given only the target text itself, by making use of the spatial distribution of the phrases (which gets thrown out in typical bag-of-word models).</p>

<p>The idea is that uninformative words and phrases (e.g., &#8220;the&#8221; and &#8220;but&#8221;) have a more random and uniform distribution than important words and phrases (e.g., names of main characters), which will tend to form clusters. (Funnily, the quantum chaos paper containing the primes graph reverses these characterizations: there, the primes are the ones that are repellent and spread out, while the random numbers are the ones that form clusters.)</p>

<p>Thus, the cluster-tendency of a phrase should be a reasonable measure of its importance.</p>

<h1>Implementation</h1>

<p>I wrote a quick Ruby script to calculate the level statistics on the first Harry Potter book.</p>

<p>Here are the top 25 unigrams extracted (ordered using the sigma_normalized statistic from the paper; I played around with the paper&#8217;s C number as well, but it didn&#8217;t give as good results):</p>

<pre><code>phrase  count   sigma_normalized
dudley  117     5.886458836973907
vernon  105     5.425462371919648
uncle   121     4.96291028279975
yeh     100     4.483340310689477
mirror  46      4.4123632519944564
platform        20      3.7698278374021283
wood    53      3.698871282864937
aunt    65      3.682665094335877
hagrid  336     3.532997146829891
ter     78      3.4935220547292394
petunia 57      3.3847272743671764
dragon  31      3.319790040957389
fang    16      3.1175440675916763
unicorn 22      3.109138726006347
bludgers        13      3.0336322222301035
o       31      2.973718095050078
malfoy  112     2.965094959118159
voldemort       31      2.957883190702609
percy   36      2.946348443571833
path    12      2.9430416987716383
dursley 54      2.9266934668788025
christmas       25      2.926196222714648
anythin 13      2.8753724174699626
quirrell        91      2.8351538706894868
crate   12      2.8171552951153913
</code></pre>

<p>It looks more impressive when you compare with the bottom 25 unigrams (filtered to counts of at least 50, to make the unigrams more comparable to those above):</p>

<pre><code>phrase  count   sigma_normalized
few     55      0.858973690529759
seemed  72      0.8660034944771483
might   54      0.8691077527153204
after   68      0.8700202208036406
way     94      0.8893865470583092
come    95      0.8897833973429081
before  107     0.9158168210798949
good    82      0.9208260024379561
suddenly        69      0.9223289856683224
only    111     0.9262797404590966
put     58      0.9320218173468643
must    57      0.932224843054271
while   60      0.9367996846919046
asked   60      0.9386801898266576
left    82      0.9509973378264027
trying  65      0.9511509393516154
saw     62      0.9524243012100968
even    105     0.9563651096306485
really  73      0.9569234100328665
voice   59      0.957685122636835
away    71      0.9611639088841413
where   100     0.9657416499349912
every   53      0.9659213894654203
harrys  113     0.9746316711628271
long    71      0.9794359046955133
</code></pre>

<p>Similarly, here are the top bigrams:</p>

<pre><code>phrase  count   sigma_normalized
uncle vernon    97      5.2050164080560615
the mirror      35      3.8911699122647163
professor mcgonagall    89      3.570490871211533
aunt petunia    52      3.3541554249859917
the library     21      3.2822285085324374
nine and        13      3.052813340938423
the twins       12      3.0493973437585256
the potters     12      2.962214857171763
the boy 26      2.86158988074687
said hagrid     89      2.76691520812392
the stone       48      2.7118270594517413
and dudley      15      2.705365860880404
the giant       15      2.682345959043539
the troll       18      2.6613701424473386
mr dursley      30      2.607508647153971
said uncle      13      2.5996349710103055
sorcerers stone 14      2.564840300637567
the quaffle     16      2.533417641354809
said dumbledore 27      2.5200215250194193
said hermione   42      2.509101121058932
and threequarters       9       2.4248478008322967
the crate       9       2.377194502184057
the cloak       22      2.368960931129895
the forest      23      2.3607725433338675
mr potter       12      2.3429313549690285
</code></pre>

<p>And the bottom bigrams (filtered to counts of at least 25):</p>

<pre><code>phrase  count   sigma_normalized
for a   38      0.6706459452115997
the way 34      0.7417616427971635
to harry        31      0.7671160112200728
a few   45      0.7699093885203201
off the 32      0.7949383886682402
to him  27      0.7958792937430593
was going       36      0.811227023007951
up to   39      0.8122786875347693
but it  32      0.8336661901803439
up the  40      0.8365345579027479
if you  34      0.8422559552361845
which was       28      0.8552087965411528
of course       35      0.8621365585531023
by the  43      0.8625237028663237
the other       52      0.8876800348745395
with a  55      0.8903321479589525
 and    51      0.9016112133824481
his face        31      0.9018554880700214
and he  47      0.9038362537361607
with the        56      0.9048625952654603
but i   32      0.9110050643857517
had to  43      0.9235463968235097
at last 28      0.9255524776918606
i know  27      0.9294114876847764
he could        47      0.9358967550241238
</code></pre>

<p>So interestingly, while the method doesn&#8217;t do a great job of capturing the most important phrases (I&#8217;m not really sure why it should, in any case, as I&#8217;d guess main characters and themes would appear pretty evenly throughout the text), it does seem to pick out minor characters and objects (like the Dudleys), which seems to make a bit more sense.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/03/19/counting-clusters/">Counting Clusters</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-19T04:22:16-07:00" pubdate data-updated="true">Mar 19<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Given a set of datapoints, we often want to know how many clusters the datapoints form. The <strong>gap statistic</strong> and the <strong>prediction strength</strong> are two practical algorithms for choosing the number of clusters.</p>

<h1>Gap Statistic</h1>

<p>The <a href="http://www.stanford.edu/~hastie/Papers/gap.pdf">gap statistic algorithm</a> works as follows:</p>

<p>For each i from 1 up to some maximum number of clusters,</p>

<ol>
<li><p>Run a k-means algorithm on the original dataset to find i clusters, and sum the distance of all points from their cluster mean. Call this sum the <strong>dispersion</strong>.</p></li>
<li><p>Generate a set of <em>reference</em> datasets (of the same size as the original). One simple way of generating a reference dataset is to sample uniformly from the original dataset&#8217;s bounding rectangle; a more sophisticated approach is take into account the original dataset&#8217;s shape by sampling, say, from a rectangle formed from the original dataset&#8217;s principal components.</p></li>
<li><p>Calculate the dispersion of each of these reference datasets, and take their mean.</p></li>
<li><p>Define the ith <strong>gap</strong> by: log(mean dispersion of reference datasets) - log(dispersion of original dataset).</p></li>
</ol>


<p>Once we&#8217;ve calculated all the gaps (we can add confidence intervals as well; see <a href="http://www.stanford.edu/~hastie/Papers/gap.pdf">the original paper</a> for the formula), we can select the number of clusters to be the one that gives the maximum gap. (Sidenote: I view the gap statistic as a very statistical-minded algorithm, since it compares the original dataset against a set of reference &#8220;control&#8221; datasets.)</p>

<p>For example, here I&#8217;ve generated three Gaussian clusters:</p>

<p><a href="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d.png"><img src="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d.png" alt="Three Gaussian Clusters" /></a></p>

<p>And running the gap statistic algorithm, we see that it correctly detects the number of clusters to be three:</p>

<p><a href="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d_gaps.png"><img src="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d_gaps.png" alt="Gap Statistic on Three Gaussian Clusters" /></a></p>

<p>For a sample R implementation of the gap statistic, see the Github repository <a href="https://github.com/echen/gap-statistic">here</a>.</p>

<h1>Prediction Strength</h1>

<p>Another cluster-counting algorithm is the <a href="http://www-stat.stanford.edu/~tibs/ftp/predstr.ps">prediction strength algorithm</a>. In contrast to the gap statistic (which, as mentioned above, I find very statistically), I see prediction strength as taking a more machine learning viewpoint, since it&#8217;s formulated as a supervised learning problem validated against a test set.</p>

<p>To calculate prediction strength, for each i from 1 up to some maximum number of clusters:</p>

<ol>
<li><p>Divide the dataset into two groups, a training set and a test set.</p></li>
<li><p>Run a k-means algorithm on each set to find i clusters.</p></li>
<li><p>For each <em>test</em> cluster, count the proportion of pairs of points in that cluster that would remain in the same cluster, if each were assigned to its closest <em>training</em> cluster mean.</p></li>
<li><p>The minimum over these proportions is the <strong>prediction strength</strong> for i clusters.</p></li>
</ol>


<p>Once we&#8217;ve calculated the prediction strength for each number of clusters, we select the number of clusters to be the maximum i such that the prediction strength for i is greater than some threshold. (The paper suggests 0.8 - 0.9 as a good threshold, and I&#8217;ve seen 0.8 work well in practice.)</p>

<p>Here&#8217;s the prediction strength algorithm run on the same example above:</p>

<p><a href="https://github.com/echen/prediction-strength/raw/master/examples/3_clusters_2d_ps.png"><img src="https://github.com/echen/prediction-strength/raw/master/examples/3_clusters_2d_ps.png" alt="Prediction Strength on Three Gaussian Clusters" /></a></p>

<p>Again, check out a sample R implementation of the prediction strength <a href="https://github.com/echen/prediction-strength">here</a>.</p>

<p>In practice, I tend to prefer using the gap statistic algorithm, since it&#8217;s a little easier to code and it doesn&#8217;t require selecting an arbitrary threshold like the prediction strength does. I&#8217;ve also found that it gives slightly better results (though the original prediction strength paper has the opposite finding).</p>

<h1>Appendix</h1>

<p>I ended up giving a brief description of two very common clustering algorithms, <strong>k-means</strong> and <strong>Gaussian mixture models</strong> in the comments, so I figured I might as well bring them up here.</p>

<h2>k-means algorithm</h2>

<p>Suppose we have a set of datapoints that we want to cluster. We want to learn two things:</p>

<ul>
<li>A description of the clusters themselves (so that if new points come in, we can assign them to a cluster).</li>
<li>Which clusters our current points fall into.</li>
</ul>


<p>We start by initializing k cluster centers (e.g., by randomly choosing k points among our datapoints). Then we repeatedly</p>

<ul>
<li><strong>Step A</strong>: Assign each datapoint to the nearest cluster center.</li>
<li><strong>Step B</strong>: Update all the cluster centers: for each cluster i, take the mean over all points currently in the cluster, and update cluster center i to be this mean.</li>
<li>(Repeat steps A and B above until the cluster assignments stop changing.)</li>
</ul>


<p>And that&#8217;s pretty much it for k-means.</p>

<h2>k-means from an EM point of View</h2>

<p>To ease the transition into Gaussian mixture models,
let&#8217;s also describe the k-means algorithm using <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM</a> language.</p>

<p>Note that if we knew for certain either 1) the exact cluster centers or 2) the cluster each point belonged to, we could trivially solve k-means, since</p>

<ul>
<li>If we knew the exact cluster centers, all we&#8217;d have to do is assign each point to its nearest cluster center, and we&#8217;d be done.</li>
<li>If we knew which cluster each point belonged to, we could pick the cluster center by simply taking the mean over all points in that cluster.</li>
</ul>


<p>The problem is that we know <em>neither</em> of these, and so we alternate between making educated guesses of each one:</p>

<ul>
<li>In A step above, we pretend that we know the cluster centers, and based off this pretense, we guess which cluster each point belongs to. (This is also known as the <strong>E step</strong> in the EM algorithm.)</li>
<li>In the B step above, we do the reverse: we pretend that we know which cluster each point belongs to, and then try to guess the cluster centers. (This is also known as the <strong>M step</strong> in EM.)</li>
</ul>


<p>Our guesses keep getting better and better, and eventually we&#8217;ll converge.</p>

<h2>Gaussian Mixture Models</h2>

<p>k-means has a <em>hard</em> notion of clustering: point X either belongs to cluster C or it doesn&#8217;t. But sometimes we want a <em>soft</em> notion instead: point X belongs to cluster C with probability p (according to a Gaussian kernel). This is where <a href="http://en.wikipedia.org/wiki/Mixture_model">Gaussian mixture modeling</a> comes in.</p>

<p>To run a GMM, we start by initializing $k$ Gaussians (say, by randomly choosing $k$ points to be the centers of the Gaussians and by setting the variance of each Gaussians to be the overall variance), and then we repeatedly:</p>

<ul>
<li><strong>E Step</strong>: Pretend we know the parameters of each Gaussian cluster, and assign each datapoint to Gaussian cluster i with appropriate probability.</li>
<li><strong>M Step</strong>: Pretend we know the probabilities that each point belongs to a given cluster. Using these probabilities, update the means and variances of each Gaussian cluster: the new mean for cluster i is the weighted mean over all points (where the weight of each point X is the probability that X belongs to cluster i), and similarly for the new variance.</li>
</ul>


<p>This is exactly like k-means in the EM formulation, except we replace the binary clustering formula with Gaussian kernels.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/03/14/prime-numbers-and-the-riemann-zeta-function/">Prime Numbers and the Riemann Zeta Function</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-14T04:28:39-07:00" pubdate data-updated="true">Mar 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Lots of people know that the <a href="http://en.wikipedia.org/wiki/Riemann_hypothesis">Riemann Hypothesis</a> has <em>something</em> to do with prime numbers, but most introductions fail to say what or why. I&#8217;ll try to give one angle of explanation.</p>

<h1>Layman&#8217;s Terms</h1>

<p>Suppose you have a bunch of friends, each with an instrument that plays at a frequency equal to the imaginary part of a zero of the Riemann zeta function. If the Riemann Hypothesis holds, you can create a song that sounds exactly at the prime-powered beats, by simply telling all your friends to play at the same volume.</p>

<h1>Mathematical Terms</h1>

<p>Let $ \pi(x) $ denote the number of primes less than or equal to x. Recall <a href="http://en.wikipedia.org/wiki/Prime_number_theorem#The_prime-counting_function_in_terms_of_the_logarithmic_integral">Gauss&#8217;s approximation</a>: $ \pi(x) \approx \int_2^x \frac{1}{\log t} \,dt $ (aka, the &#8220;probability that a number n is prime&#8221; is approximately $ \frac{1}{\log n} $).</p>

<p>Riemann improved on Gauss&#8217;s approximation by discovering an <em>exact</em> formula $ P(x) = A(x) - E(x) $ for counting the primes, where</p>

<ul>
<li>$ P(x) = \sum_{p^k &lt; x} \frac{1}{k} $ performs a weighted count of the prime powers less than or equal to x. [Think of this as a generalization of the prime counting function.]</li>
<li>$ A(x) = \int_0^x \frac{1}{\log t} \,dt+ \int_x^{\infty} \frac{1}{t(t^2  -1) \log t} \,dt $ $ - \log 2 $ is a kind of generalization of Gauss&#8217;s approximation.</li>
<li>$ E(x) = \sum_{z : \zeta(z) = 0} \int_0^{x^z} \frac{1}{\log t} \,dt $ is an error-correcting factor that depends on the zeroes of the Riemann zeta function.</li>
</ul>


<p>In other words, if we use a simple Gauss-like approximation to the distribution of the primes, the zeroes of the Riemann zeta function sweep up after our errors.</p>

<p>Let&#8217;s dig a little deeper. Instead of using Riemann&#8217;s formula, I&#8217;m going to use an equivalent version</p>

<p>$$ \psi(x) = (x + \sum_{n = 1}^{\infty} \frac{x^{-2n}}{2n} - \log 2\pi) - \sum_{z : \zeta(z) = 0} \frac{x^z}{z} $$</p>

<p>where  $ \psi(x) = \sum_{p^k \le x} \log p $. Envisioning this formula to be in the same $P(x) = A(x) - E(x)$ form as above, this time where</p>

<ul>
<li>$ P(x) = \psi(x) = \sum_{p^k \le x} \log p $ is another kind of count of the primes.</li>
<li>$ A(x) = x + \sum_{n = 1}^{\infty} \frac{x^{-2n}}{2n} - \log 2\pi $ is another kind of approximation to $P(x)$.</li>
<li>$ E(x) = \sum_{z : \zeta(z) = 0} \frac{x^z}{z} $ is another error-correction factor that depends on the zeroes of the Riemann zeta function.</li>
</ul>


<p>we can again interpret it as an error-correcting formula for counting the primes.</p>

<p>Now since $ \psi(x) $ is a step function that jumps at the prime powers, its derivative $ \psi&#8217;(x) $ has spikes at the prime powers and is zero everywhere else. So consider</p>

<p>$$ \psi&#8217;(x) = 1 - \frac{1}{x(x^2 - 1)} - \sum_z x^{z-1} $$</p>

<p>It&#8217;s well-known that the zeroes of the Riemann zeta function are symmetric about the real axis, so the (non-trivial) zeroes come in conjugate pairs $ z, \bar{z} $. But $ x^{z-1} + x^{\bar{z} - 1} $ is just a wave whose amplitude depends on the real part of z and whose frequency depends on the imaginary part (i.e., if $ z = a + bi $, then $ x^{z-1} + x^{\bar{z}-1} = 2x^{a-1} cos (b \log x) $), which means $ \psi&#8217;(x) $ can be decomposed into a sum of zeta-zero waves. Note that because of the $2x^{a-1}$ term in front, the amplitude of these waves depends only on the real part $a$ of the conjugate zeroes.</p>

<p>For example, here are plots of $ \psi&#8217;(x) $ using 10, 50, and 200 pairs of zeroes:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/10.png"><img src="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/10.png" alt="10 Pairs" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/50.png"><img src="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/50.png" alt="50 Pairs" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/200.png"><img src="http://dl.dropbox.com/u/10506/blog/riemann-hypothesis/200.png" alt="50 Pairs" /></a></p>

<p>So when the Riemann Hypothesis says that all the non-trivial zeroes have real part 1/2, it&#8217;s hypothesizing that the non-trivial zeta-zero waves have equal amplitude, i.e., they make equal contributions to counting the primes.</p>

<p><strong>In Fourier-poetic terms</strong>, when Flying Spaghetti Monster composed the music of the primes, he built the notes out of the zeroes of the Riemann zeta function. If the Riemann Hypothesis holds, he made all the non-trivial notes equally loud.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/03/14/hacker-news-analysis/">Hacker News Analysis</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-14T04:27:32-07:00" pubdate data-updated="true">Mar 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I was playing around with the <a href="http://api.ihackernews.com/?db">Hacker News database</a> <a href="http://ronnieroller.com/">Ronnie Roller</a> made (thanks!), so I thought I&#8217;d post some of the things I looked at.</p>

<h1>Activity on the Site</h1>

<p>My first question was how activity on the site has increased over time. I looked at number of posts, points on posts, comments on posts, and number of users.</p>

<h2>Posts</h2>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/posts_by_month.png" alt="Hacker News Posts by Month" /></p>

<p>This looks like a strong linear fit, with an increase of 292 posts every month.</p>

<h2>Comments</h2>

<p>For comments, I fit a quadratic regression:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/comments_by_month.png" alt="Hacker News Comments by Month" /></p>

<h2>Points</h2>

<p>A quadratic regression was also a better fit for points by month:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/points_by_month.png" alt="Hacker News Points by Month" /></p>

<h2>Users</h2>

<p>And again for the number of distinct users with a submission:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/users.png" alt="Hacker News Users by Month" /></p>

<h1>Points and Comments</h1>

<p>My next question was how points and comments related. Intuitively, posts with more points should have more comments, but it&#8217;s nice to check (maybe really good posts are kind of boring, so don&#8217;t lead to much discussion).</p>

<p>First, I plotted the points and comments of each individual post:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/all_points_vs_comments.png" alt="All Points vs. Comments" /></p>

<p>As expected, theres an overall positive correlation between points and comments. Interestingly, there are quite a few high-points posts with no comments.</p>

<p>The plots quite noisy, though, so lets try cleaning it up a bit, by taking the median number of comments per points level (and removing posts at the higher end, where we have little data):</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/points_vs_median_comments.png" alt="Points vs. Median Comments" /></p>

<p>We see that posts with more points do tend to have more comments. Also, variance in number of comments is indicated by size and color, so (unsurprisingly) posts with more points have larger variance in their number of comments.</p>

<h1>Quality of Posts</h1>

<p>Another question was whether the quality of posts has degraded over time.</p>

<p>First, I computed a normalized &#8220;score&#8221; for each post, where a post&#8217;s score is defined as the number of points divided by the number of distinct users who made a submission in the same month. (The denominator is a rough proxy for the number of active users, and the goal of the score is to provide a way to compare posts across time.)</p>

<p>While the median score has declined over time (as perhaps should be expected, since only a fixed number of items can reach the front page):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/hn-analysis/median-score.png"><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/median-score.png" alt="Median Score" /></a></p>

<p>the absolute <em>number</em> of quality posts, defined as posts with a score greater than the (admittedly arbitrarily chosen) threshold 0.01, has increased (until possibly a dip starting in 2010):</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/num_quality_posts2.png" alt="Number of Quality Posts" /></p>

<p>(Of course, without some further analysis, it&#8217;s not clear how well this score measures quality of posts, so take these numbers with a grain of salt.)</p>

<h1>Company Trends</h1>

<p>Also, I wanted to see how certain topics have trended over time, so I looked at how mentions of some of the big-name companies (Google, Facebook, Microsoft, Yahoo, Twitter, Apple) have changed. For each company, I plotted the percentage of posts with the company&#8217;s name in the title, and also made a smoothed plot comparing all six at the end. Note that Microsoft and Yahoo seem to be trending slightly downward, and Apple seems to be trending upward.</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_microsoft.png" alt="Mentions of Microsoft" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_yahoo.png" alt="Mentions of Yahoo" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_google.png" alt="Mentions of Google" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_facebook.png" alt="Mentions of Facebook" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_twitter.png" alt="Mentions of Twitter" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_apple.png" alt="Mentions of Apple" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/all_trends2.png" alt="All Trends" /></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/03/14/least-angle-regression-for-the-hungry-layman/">A Layman&#8217;s Explanation of Least Angle Regression</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-14T04:27:10-07:00" pubdate data-updated="true">Mar 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>(I wrote up a more mathematical explanation of least angle regression <a href="http://blog.echen.me/2011/04/21/a-mathematical-introduction-to-least-angle-regression/">here</a>.)</p>

<p>Suppose you&#8217;re at a buffet. You don&#8217;t want to just grab everything and overeat (overfit), so how do you decide which dishes to take? Here are some possibilities.</p>

<h1>Forward Selection</h1>

<p>On your first trip to the buffet, take a plateful of your favorite dish, bring it back, and eat it. On your second trip, take a plateful of the dish that <em>now</em> looks most appetizing, bring it back, and eat it. And so on.</p>

<p>The problem with this method is the following: suppose your favorite food is spaghetti, followed closely by macaroni. Ideally, you&#8217;d like to take half a plate of spaghetti and slightly less than half a plate of macaroni, but under this method, you have to first take and eat a plateful of spaghetti, and now you&#8217;re sick of pasta and don&#8217;t want macaroni anymore.</p>

<h1>Forward Stagewise</h1>

<p>To remedy the greediness of the above method, another option is to take smaller morsels at a time. On your first trip to the buffet, you take a thimbleful of your favorite dish; on your next trip to the buffet, you take a thimbleful of the currently most appetizing dish; and so on again.</p>

<p>Now, after getting your thimbleful of spaghetti, pasta still looks delicious to you, so you can get your thimbleful of macaroni as well.</p>

<p>The problem with this method is that because you&#8217;re only eating a thimbleful at a time, you have to make many trips to the buffet and so dinner takes forever.</p>

<h1>Least Angle Regression</h1>

<p>A much more efficient method works as follows. Suppose your favorite dishes are, in order, spaghetti, macaroni, salad, and chili. On your first trip, you grab a bunch of spaghetti. You know that as you eat spaghetti, you start to get slightly sick of it, so it becomes less and less appetizing, until eventually it becomes just as appetizing as macaroni (but still more appetizing than salad and chili). So only grab an amount X of spaghetti, so that after eating X, spaghetti and macaroni are equally appetizing.</p>

<p>On your second trip, spaghetti and macaroni are equally appetizing. Grab both spaghetti and macaroni, in proportions such that spaghetti and macaroni stay equally appetizing while you eat. Again, you know exactly how much spaghetti and macaroni you can eat until salad becomes just as appetizing, so only grab this amount.</p>

<p>On your third trip, spaghetti, macaroni, and salad look equally delicious. Again, grab the three foods in proportions that stay equally appetizing while you eat, and only grab enough to make chili look just as tasty.</p>

<p>And so on.</p>

<p>Note that:</p>

<ul>
<li><p>This method works better than forward selection, because we get to eat both spaghetti and macaroni.</p></li>
<li><p>This method works much faster than forward stagewise, because we make much fewer trips. (If we want to eat $latex n$ dishes, we only need to make $latex n$ trips.)</p></li>
<li><p>We&#8217;re always eating whatever looks most appetizing to us.</p></li>
</ul>


<h1>Reversing the Analogy</h1>

<p>Replace buffet with linear regression and dishes with variables, and you now have three tasty model selection methods to choose from.</p>

<p>For a more mathematical explanation of least angle regression, see <a href="http://edchedch.wordpress.com/2011/04/21/a-mathematical-introduction-to-least-angle-regression/">here</a>.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/page/4/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/page/2/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Edwin Chen</h1>
  
  <p>Data scientist at Twitter. Previously math and linguistics at MIT, quantitative trading at Clarium Capital.</p>
  
  <p>I like math, statistics, machine learning, and linguistics.</p>

  <p>Email: hello[at]echen.me<br/>
  Twitter: <a href="https://twitter.com/#!/echen">@echen</a><br/>
  Other: <a href="https://github.com/echen">Github</a>, <a href="https://plus.google.com/113804726252165471503/">Google+</a>, <a href="http://www.linkedin.com/in/edwinchen1">LinkedIn</a>, <a href="http://quora.com/edwin-chen-1">Quora</a></p>
</section><section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook&#8217;s User Recommendation Contest on Kaggle</a>
      </li>
    
      <li class="post">
        <a href="/2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter</a>
      </li>
    
      <li class="post">
        <a href="/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">Making the Most of Mechanical Turk: Tips and Best Practices</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant interactive visualization with d3 + ggplot2</a>
      </li>
    
      <li class="post">
        <a href="/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/">Movie recommendations and more via MapReduce and Scalding</a>
      </li>
    
      <li class="post">
        <a href="/2012/01/17/quick-introduction-to-ggplot2/">Quick Introduction to ggplot2</a>
      </li>
    
      <li class="post">
        <a href="/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a>
      </li>
    
      <li class="post">
        <a href="/2011/10/24/winning-the-netflix-prize-a-summary/">Winning the Netflix Prize: A Summary</a>
      </li>
    
      <li class="post">
        <a href="/2011/09/29/stuff-harvard-people-like/">Stuff Harvard People Like</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("echen", 5, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/echen" class="twitter-follow-button" data-show-count="true">Follow @echen</a>
  
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Edwin Chen -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'edwinchen';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>